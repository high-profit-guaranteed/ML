{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79031c45-67b4-4ad3-92e6-dc2691520a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-05 00:00:00 2024-04-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Publish date Published: Apr 05, 2024, 2:55 AM could not be resolved to UTC\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "import time\n",
    "import requests\n",
    "import feedparser\n",
    "from goose3 import Goose\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "\n",
    "# Get links from google news\n",
    "\n",
    "# def getDateAndLinks(keyword, term):\n",
    "#     url = 'https://news.google.com/rss/search?q=' + keyword + \\\n",
    "#         '+when:' + str(term) + 'd&hl=en-US&gl=US&ceid=US:en'\n",
    "#     text = getData(url)\n",
    "#     datas = feedparser.parse(text).entries\n",
    "#     links = []\n",
    "#     driver = webdriver.Chrome()\n",
    "#     for data in datas:\n",
    "#         date = data.published\n",
    "#         driver.get(data.link)\n",
    "#         soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "#         linkTag = soup.find('a')\n",
    "#         if linkTag == None:\n",
    "#             continue\n",
    "#         links.append([date, linkTag.text])\n",
    "#     driver.quit()\n",
    "#     return links\n",
    "\n",
    "# https://www.google.com/search?q=google+search+date&client=safari&sca_esv=3d20c04c2adbaa4c&sca_upv=1&rls=en&sxsrf=ACQVn090P0KZuMNbSEHive6QuI_YyEFBaw%3A1712733717852&source=lnt&tbs=cdr%3A1%2Ccd_min%3A4%2F2%2F2024%2Ccd_max%3A4%2F10%2F2024&tbm=\n",
    "def getDateAndLinks(keyword, t1, t2):\n",
    "    url = 'https://news.google.com/rss/search?q=' + keyword + \\\n",
    "        '+after:' + t1 + '+before:' + t2 + '&ceid=US:en&hl=en-US&gl=US'\n",
    "    text = getData(url)\n",
    "    datas = feedparser.parse(text).entries\n",
    "    links = []\n",
    "    driver = webdriver.Chrome()\n",
    "    for data in datas:\n",
    "        date = data.published\n",
    "        driver.get(data.link)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        linkTag = soup.find('a')\n",
    "        if linkTag == None:\n",
    "            continue\n",
    "        links.append([date, linkTag.text])\n",
    "    driver.quit()\n",
    "    return links\n",
    "\n",
    "# def getDateAndLinks(keyword):\n",
    "#     url = 'https://news.google.com/rss/search?q=' + keyword + \\\n",
    "#         '&ceid=US:en&hl=en-US&gl=US'\n",
    "#     text = getData(url)\n",
    "#     datas = feedparser.parse(text).entries\n",
    "#     links = []\n",
    "#     driver = webdriver.Chrome()\n",
    "#     for data in datas:\n",
    "#         date = data.published\n",
    "#         driver.get(data.link)\n",
    "#         soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "#         linkTag = soup.find('a')\n",
    "#         if linkTag == None:\n",
    "#             continue\n",
    "#         links.append([date, linkTag.text])\n",
    "#     driver.quit()\n",
    "#     return links\n",
    "\n",
    "# Get data from link\n",
    "\n",
    "\n",
    "def getData(link):\n",
    "    try:\n",
    "        response = requests.get(link)\n",
    "        return response.text\n",
    "    except:\n",
    "        return getData(link)\n",
    "# Get article from link\n",
    "\n",
    "\n",
    "def getArticle(link, g):\n",
    "    try:\n",
    "        article = g.extract(url=link)\n",
    "        return article.cleaned_text\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "# Get news from keyword and term\n",
    "\n",
    "\n",
    "# def getNews(keyword, term):\n",
    "#     g = Goose({'browser_user_agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_2)',\n",
    "#               'parser_class': 'soup', 'strict': False})\n",
    "#     links = getDateAndLinks(keyword, term)\n",
    "#     articles = []\n",
    "#     for link in links:\n",
    "#         # print(link)\n",
    "#         article = getArticle(link[1], g)\n",
    "#         if article == \"\" or article == None or article == \"Please enable JS and disable any ad blocker\" or article == \"Keep me logged in from this computer.\" or article.startswith(\"Access to this page has been denied because we believe you are using automation tools to browse the website.\"):\n",
    "#             continue\n",
    "#         articles.append(str(link[0]) + \"\\n\\n\" + article)\n",
    "#     return articles\n",
    "\n",
    "# def getNews(keyword, t1, t2):\n",
    "#     g = Goose({'browser_user_agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_2)',\n",
    "#               'parser_class': 'soup', 'strict': False})\n",
    "#     links = getDateAndLinks(keyword, t1, t2)\n",
    "#     articles = []\n",
    "#     for link in links:\n",
    "#         # print(link)\n",
    "#         article = getArticle(link[1], g)\n",
    "#         if article == \"\" or article == None or article == \"Please enable JS and disable any ad blocker\" or article == \"Keep me logged in from this computer.\" or article.startswith(\"Access to this page has been denied because we believe you are using automation tools to browse the website.\"):\n",
    "#             continue\n",
    "#         articles.append(str(link[0]) + \"\\n\\n\" + article)\n",
    "#     return articles\n",
    "\n",
    "# def getNews(keyword):\n",
    "#     g = Goose({'browser_user_agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_2)',\n",
    "#               'parser_class': 'soup', 'strict': False})\n",
    "#     links = getDateAndLinks(keyword)\n",
    "#     articles = []\n",
    "#     for link in links:\n",
    "#         # print(link)\n",
    "#         article = getArticle(link[1], g)\n",
    "#         if article == \"\" or article == None or article == \"Please enable JS and disable any ad blocker\" or article == \"Keep me logged in from this computer.\" or article.startswith(\"Access to this page has been denied because we believe you are using automation tools to browse the website.\"):\n",
    "#             continue\n",
    "#         articles.append(str(link[0]) + \"\\n\\n\" + article)\n",
    "#     return articles\n",
    "\n",
    "# def getNews(keyword, tS1, tS2):\n",
    "#     g = Goose({'browser_user_agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_2)',\n",
    "#               'parser_class': 'soup', 'strict': False})\n",
    "#     # t1 = datetime.fromtimestamp(time.mktime(time.strptime(tS1, \"%Y-%m-%d\")))\n",
    "#     # t2 = datetime.fromtimestamp(time.mktime(time.strptime(tS2, \"%Y-%m-%d\")))\n",
    "#     t1 = datetime.strptime(tS1, \"%Y-%m-%d\")\n",
    "#     t2 = datetime.strptime(tS2, \"%Y-%m-%d\")\n",
    "#     linkList = []\n",
    "#     while t1 <= t2:\n",
    "#         timeStr1 = t1.strftime(\"%Y-%m-%d\")\n",
    "#         timeStr2 = (t1 + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "#         print(t1, timeStr1)\n",
    "#         links = getDateAndLinks(keyword, timeStr1, timeStr2)\n",
    "#         for link in links:\n",
    "#             linkList.append(link)\n",
    "#         # print(links)\n",
    "#         t1 += timedelta(days=2)\n",
    "#     articles = []\n",
    "#     for link in linkList:\n",
    "#         # print(link)\n",
    "#         article = getArticle(link[1], g)\n",
    "#         if article == \"\" or article == None or article == \"Please enable JS and disable any ad blocker\" or article == \"Keep me logged in from this computer.\" or article.startswith(\"Access to this page has been denied because we believe you are using automation tools to browse the website.\"):\n",
    "#             continue\n",
    "#         articles.append(str(link[0]) + \"\\n\\n\" + article)\n",
    "#     return articles\n",
    "\n",
    "def saveNews(keyword, tS1, tS2, path):\n",
    "    g = Goose({'browser_user_agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_2)',\n",
    "              'parser_class': 'soup', 'strict': False})\n",
    "    # t1 = datetime.fromtimestamp(time.mktime(time.strptime(tS1, \"%Y-%m-%d\")))\n",
    "    # t2 = datetime.fromtimestamp(time.mktime(time.strptime(tS2, \"%Y-%m-%d\")))\n",
    "    t1 = datetime.strptime(tS1, \"%Y-%m-%d\")\n",
    "    t2 = datetime.strptime(tS2, \"%Y-%m-%d\")\n",
    "    linkList = []\n",
    "    while t1 <= t2:\n",
    "        timeStr1 = t1.strftime(\"%Y-%m-%d\")\n",
    "        timeStr2 = (t1 + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "        print(t1, timeStr1)\n",
    "        links = getDateAndLinks(keyword, timeStr1, timeStr2)\n",
    "        for link in links:\n",
    "            linkList.append(link)\n",
    "        # print(links)\n",
    "        t1 += timedelta(days=2)\n",
    "    # articles = []\n",
    "    with open(path + 'header.txt', 'r', encoding='UTF-8') as headerFile:\n",
    "        index = int(headerFile.readline())\n",
    "    for link in linkList:\n",
    "        index += 1\n",
    "        # print(link)\n",
    "        article = getArticle(link[1], g)\n",
    "        if article == \"\" or article == None or article == \"Please enable JS and disable any ad blocker\" or article == \"Keep me logged in from this computer.\" or article.startswith(\"Access to this page has been denied because we believe you are using automation tools to browse the website.\"):\n",
    "            continue\n",
    "        # articles.append(str(link[0]) + \"\\n\\n\" + article)\n",
    "        with open(path + str(index) + '.txt', 'w', encoding='UTF-8') as outfile:\n",
    "            outfile.write(str(link[0]) + \"\\n\\n\" + article)\n",
    "    with open(path + 'header.txt', 'w', encoding='UTF-8') as headerFile:\n",
    "        headerFile.write(str(index+1) + '\\n')\n",
    "    # return articles\n",
    "\n",
    "# Parse time from string\n",
    "#   ex) \"Tue, 02 Apr 2024 13:07:16 GMT\"\n",
    "#         -> time.struct_time(tm_year=2024, tm_mon=4, tm_mday=2, tm_hour=13, tm_min=7, tm_sec=16, tm_wday=1, tm_yday=93, tm_isdst=-1)\n",
    "#   ref) https://docs.python.org/ko/3/library/datetime.html#strftime-strptime-behavior\n",
    "\n",
    "\n",
    "def parseTime(timeStr):\n",
    "    timeObj = time.strptime(timeStr, \"%a, %d %b %Y %H:%M:%S %Z\")\n",
    "    return timeObj\n",
    "\n",
    "\n",
    "saveNews('ASML', \"2024-04-05\", \"2024-04-05\", \"./test_ASML/\")\n",
    "# saveNews('TSLA', \"2024-05-11\", \"2024-05-14\", \"./test_TSLA/\")\n",
    "\n",
    "# # ---------------\n",
    "# # | 파일로 저장하기 |\n",
    "# # ---------------\n",
    "# # getDateAndLinks 로 구글 뉴스에서 키워드와 기간을 입력받아 해당 기사들의 링크를 반환\n",
    "# g = Goose({'browser_user_agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_2)', 'parser_class':'soup', 'strict': False})\n",
    "# links = getDateAndLinks('TSLA', 1)\n",
    "\n",
    "# # getArticle 로 링크를 입력받아 해당 기사의 본문을 ./articles/ 폴더 안에 파일로 저장\n",
    "# for index in range(len(links)):\n",
    "#   with open('./articles/' + str(index+1) + '.txt', 'w') as outfile:\n",
    "#     outfile.write(getArticle(links[index][1], g))\n",
    "\n",
    "# # ----------------\n",
    "# # | 리스트로 저장하기 |\n",
    "# # ----------------\n",
    "# articles = getNews('TSLA', 1)\n",
    "# # print(articles)\n",
    "# print(len(articles))\n",
    "\n",
    "\n",
    "# -----------------\n",
    "# | 파일로 저장하기 2 |\n",
    "# -----------------\n",
    "# saveNews('TSLA', \"2024-03-05\", \"2024-04-04\", \"./test3/\")\n",
    "\n",
    "\n",
    "\n",
    "# # -----------------\n",
    "# # | 파일로 저장하기 3 |\n",
    "# # -----------------\n",
    "# articles = getNews('TSLA')\n",
    "# for index in range(len(articles)):\n",
    "#     with open('./test2/' + str(index+1) + '.txt', 'w') as outfile:\n",
    "#         outfile.write(articles[index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efc32ca-f9b7-4db0-bfdd-ac37e5d85747",
   "metadata": {},
   "source": [
    "뉴스기사 뽑아오는 다음날부터 출력되는 현상있음 인지해야됨\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1d10f2-70b7-4923-8049-699d7a9bfcd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
