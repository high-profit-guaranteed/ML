{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7b0605f-3042-4cb1-8bf1-30acccf87795",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chan\\anaconda3\\envs\\ng\\lib\\site-packages\\yfinance\\base.py:48: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  _empty_series = pd.Series()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import logging\n",
    "from prophet import Prophet\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import pandas_datareader.data as web # 주식 데이터를 얻어오기 위해 사용\n",
    "import datetime # 시간 처리\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "import FinanceDataReader as fdr\n",
    "from prophet import Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a30b9a5-7426-473b-bdaf-14d920c47771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bertcode_predict(ticker, start_date, end_date_1):\n",
    "    ticker = ticker\n",
    "    #start_date 21-04-06 / end_date_1 24-05-09\n",
    "    start_date = start_date\n",
    "    end_date = end_date_1\n",
    "\n",
    "    start_year, start_month, start_day = start_date.split('-')\n",
    "    end_year, end_month, end_day = end_date.split('-')\n",
    "    \n",
    "    start_year = int(start_year)\n",
    "    start_month = int(start_month)\n",
    "    start_day = int(start_day)\n",
    "\n",
    "    end_year = int(end_year)\n",
    "    end_month = int(end_month)\n",
    "    end_day = int(end_day)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    max_seq_len = 128\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    path = f\"C:/Users/chan/Documents/GitHub/ML/test_{ticker}/\"\n",
    "    # path = \"C:/Users/chan/Documents/GitHub/ML/test10_AVGO/\"\n",
    "    news_df = pd.DataFrame(columns=[\"title\", \"content\"])\n",
    "\n",
    "    \n",
    "    for txts in os.listdir(path):\n",
    "        full_path = os.path.join(path, txts)  # 파일 전체 경로 생성\n",
    "        if full_path.endswith('header.txt'):\n",
    "            continue\n",
    "        if os.path.isfile(full_path):  # 파일인지 확인\n",
    "            with open(full_path, \"r\", encoding=\"utf-8\") as txt_file:\n",
    "                title = txt_file.readline().strip()\n",
    "                content = txt_file.read().replace('\\n', ' ')\n",
    "                # DataFrame 생성 후 concat 함수를 사용하여 추가\n",
    "                new_row = pd.DataFrame({\"title\": [title], \"content\": [content]})\n",
    "                news_df = pd.concat([news_df, new_row], ignore_index=True)\n",
    "    inputs = [content for content in news_df['content']]\n",
    "\n",
    "    # 입력 데이터를 BERT 모델의 입력 형식에 맞게 변환\n",
    "    max_length = 128\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for content in inputs:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            content,                    # content\n",
    "                            add_special_tokens = True,  # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = max_length,           # Pad & truncate all sentences\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors\n",
    "                       )\n",
    "        \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    predict_model = torch.load(\"bert_model_loss0.34.pth\", map_location=torch.device('cpu'))\n",
    "    predict_model.to(torch.device('cpu'))\n",
    "\n",
    "    predicted_labels = []\n",
    "\n",
    "    for inputs in tqdm(zip(input_ids, attention_masks)):\n",
    "        input_ids = inputs[0].to(torch.device('cpu'))\n",
    "        attention_mask = inputs[1].to(torch.device('cpu'))\n",
    "        output = predict_model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "        ps = F.softmax(output.logits, dim=1)\n",
    "        top_p, top_class = ps.topk(1, dim=1)\n",
    "        predicted_labels.append(top_class.item())\n",
    "    \n",
    "    predict_df = pd.DataFrame({'predicted_label': predicted_labels})\n",
    "    news_df[\"predict\"] = predict_df\n",
    "    news_df.to_csv('predicted_news.csv', index=False)\n",
    "\n",
    "    news_df['date'] = pd.to_datetime(news_df['title']).dt.date  # 날짜 추출\n",
    "    news_df['time'] = pd.to_datetime(news_df['title']).dt.time  # 시간 추출\n",
    "    \n",
    "    # 날짜별로 그룹화하고, 'predict' 열에 대한 평균을 계산\n",
    "    grouped_df = news_df.groupby('date').agg({\n",
    "        'title': lambda x: x.tolist(),\n",
    "        'content': lambda x: x.tolist(),\n",
    "        'predict': lambda x: round(x.mean(), 3)  \n",
    "    })\n",
    "    \n",
    "    grouped_df.reset_index(inplace=True)  # 인덱스 리셋\n",
    "    \n",
    "    # 결과 출력\n",
    "    grouped_df[['date', 'title', 'content', 'predict']]\n",
    "\n",
    "    result_df = grouped_df[['date', 'predict']]\n",
    "\n",
    "    news_df['date'] = pd.to_datetime(news_df['title']).dt.date  # 날짜 추출\n",
    "    news_df['time'] = pd.to_datetime(news_df['title']).dt.time  # 시간 추출\n",
    "    \n",
    "    # 'date' 열을 datetime 형식으로 다시 변환\n",
    "    news_df['date'] = pd.to_datetime(news_df['date'])\n",
    "    \n",
    "    # 날짜별로 그룹화하고, 'predict' 열에 대한 평균을 계산\n",
    "    grouped_df = news_df.groupby('date').agg({\n",
    "        'title': lambda x: x.tolist(),\n",
    "        'content': lambda x: x.tolist(),\n",
    "        'predict': lambda x: round(x.mean(), 3)  \n",
    "    })\n",
    "    \n",
    "    grouped_df.reset_index(inplace=True)  # 인덱스 리셋\n",
    "    \n",
    "    # 모든 날짜를 포함하는 날짜 범위 생성\n",
    "    all_dates = pd.date_range(start=grouped_df['date'].min(), end=grouped_df['date'].max(), freq='D')\n",
    "    # 새로운 DataFrame 생성 후 기존 데이터와 병합\n",
    "    complete_df = pd.DataFrame(all_dates, columns=['date'])\n",
    "    complete_df['date'] = pd.to_datetime(complete_df['date'])  # 날짜를 datetime으로 변환\n",
    "    complete_df = complete_df.merge(grouped_df, on='date', how='left')\n",
    "    # 누락된 'predict' 값을 이전 값으로 채우기\n",
    "    complete_df['predict'] = complete_df['predict'].fillna(method='ffill')\n",
    "    \n",
    "    # 'date'와 'predict' 열만 선택하여 결과 출력\n",
    "    result_df = complete_df[['date', 'predict']]\n",
    "\n",
    "    start_date = datetime.datetime( start_year, start_month, start_day )\n",
    "    end_date = datetime.datetime( end_year, end_month, end_day+1)\n",
    "    \n",
    "    # 주어진 범위 내에 있는 날짜 생성\n",
    "    missing_dates = pd.date_range(start=start_date, end=end_date).difference(result_df['date'])\n",
    "    \n",
    "    # 새로운 날짜와 예측값 추가\n",
    "    for date in missing_dates:\n",
    "        result_df = result_df.append({'date': date, 'predict': 1.0}, ignore_index=True)\n",
    "    \n",
    "    # 날짜 기준으로 데이터프레임 정렬\n",
    "    result_df = result_df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "    def makeStockChart(code, sDay, eDay):\n",
    "        data = yf.download(code, start=sDay, end=eDay)\n",
    "\n",
    "    code_name = ticker\n",
    "    makeStockChart(code_name, start_date, end_date) #기간받아오기\n",
    "    \n",
    "    datas = yf.download(code_name, start=start_date, end=end_date)\n",
    "\n",
    "    dic = {\n",
    "        'ds' : datas.index,\n",
    "        'y' : datas.Close,\n",
    "        # 'volume': datas.Volume\n",
    "        \n",
    "    }\n",
    "    \n",
    "    finance_df = pd.DataFrame( dic )\n",
    "    \n",
    "    #인덱스 초기화(원본 까지 적용)\n",
    "    finance_df.reset_index( inplace=True )\n",
    "    \n",
    "    del finance_df['Date']\n",
    "\n",
    "    date_range = pd.date_range(start=start_date, end=end_date)\n",
    "    df_date_range = pd.DataFrame(date_range, columns=['date'])\n",
    "    df_filtered = df_date_range[~df_date_range['date'].isin(finance_df['ds'])]\n",
    "\n",
    "    common_dates = result_df[result_df['date'].isin(df_filtered['date'])]['date']\n",
    "    result_filtered = result_df[~result_df['date'].isin(common_dates)]\n",
    "\n",
    "    result_filtered.reset_index(inplace=True)\n",
    "\n",
    "    result_filtered_future = result_filtered.copy()\n",
    "    result_filtered_future = result_filtered_future.drop(columns=['index'])\n",
    "\n",
    "    # 'predict' 열에만 평균 값 추가\n",
    "    mean_last_seven = result_filtered_future['predict'].iloc[-7:].mean()\n",
    "    # mean_last_seven_1 = result_filtered_future['volume'].iloc[-7:].mean()\n",
    "    # 다음 날짜 계산\n",
    "    last_date = result_filtered_future['date'].iloc[-1]\n",
    "    next_date = pd.to_datetime(last_date) + pd.DateOffset(days=1)\n",
    "    \n",
    "    # 데이터프레임에 새로운 행 추가\n",
    "    new_row = {'date': next_date, 'predict': round(mean_last_seven, 3)}  # 소수점 세 자리까지 반올림\n",
    "    result_filtered_future = result_filtered_future.append(new_row, ignore_index=True)\n",
    "    \n",
    "    # 'predict' 열의 소수점 자리 수 설정\n",
    "    result_filtered_future['predict'] = result_filtered_future['predict'].round(3)\n",
    "    \n",
    "    finance_df = pd.concat([finance_df, result_filtered['predict']], axis=1)\n",
    "\n",
    "    return finance_df, result_filtered_future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dbbf923-5b06-4f3f-b3da-801af3969de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prophet_model_predict(ticker, start_date, end_date_1):\n",
    "    \n",
    "    # 저장된 모델 파라미터를 불러오는 함수\n",
    "    ticker = ticker\n",
    "    start_date = start_date\n",
    "    end_date_1 = end_date_1 \n",
    "    finance_df,result_filtered_future = bertcode_predict(ticker,start_date,end_date_1)\n",
    "    def load_prophet_model_parameters(file_path):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            model_params = pickle.load(f)\n",
    "        return model_params\n",
    "        \n",
    "    file_path = f\"prophet_model_parameters_{ticker}.pkl\"\n",
    "    loaded_model_parameters = load_prophet_model_parameters(file_path)\n",
    "    \n",
    "    # Prophet 모델 생성 및 파라미터 설정\n",
    "    loaded_model = Prophet(\n",
    "        changepoint_prior_scale=loaded_model_parameters['changepoint_prior_scale'],\n",
    "        seasonality_prior_scale=loaded_model_parameters['seasonality_prior_scale'],\n",
    "        holidays_prior_scale=loaded_model_parameters['holidays_prior_scale'],\n",
    "        seasonality_mode=loaded_model_parameters['seasonality_mode'],\n",
    "        changepoint_range=loaded_model_parameters['changepoint_range'],\n",
    "        yearly_seasonality=loaded_model_parameters['yearly_seasonality'],\n",
    "        weekly_seasonality=loaded_model_parameters['weekly_seasonality'],\n",
    "        daily_seasonality=loaded_model_parameters['daily_seasonality'],\n",
    "        growth=loaded_model_parameters['growth'],\n",
    "        n_changepoints=loaded_model_parameters['n_changepoints']\n",
    "    )\n",
    "    loaded_model.add_country_holidays(country_name='US')\n",
    "    \n",
    "    loaded_model.add_regressor('predict')\n",
    "    \n",
    "    # 데이터 학습\n",
    "    loaded_model.fit(finance_df)\n",
    "    future = loaded_model.make_future_dataframe( periods = 1)\n",
    "    future['predict'] = result_filtered_future['predict'].values\n",
    "    # 주가 예측\n",
    "    forecast = loaded_model.predict(future)\n",
    "\n",
    "    return finance_df, forecast "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37a346af-d87f-444c-bdf3-0848419f5fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_trading(ticker, start_date, end_date_1, end_date_2,initial_capital):\n",
    "\n",
    "    start_date = start_date\n",
    "    end_date_1 = end_date_1\n",
    "    end_date_2 = end_date_2\n",
    "    finance_df,forecast = prophet_model_predict(ticker, start_date, end_date_1)\n",
    "    # 입력된 날짜 범위로 데이터 필터링\n",
    "    filtered_df = finance_df[(finance_df['ds'] >= start_date) & (finance_df['ds'] <= end_date_1)]\n",
    "    filtered_df_forecast = forecast[(forecast['ds'] >= start_date) & (forecast['ds'] <= end_date_2)]\n",
    "\n",
    "    # 현재 자본 및 보유 주식 수 초기화\n",
    "    current_capital = initial_capital\n",
    "    shares_owned = 0\n",
    "\n",
    "    # 현재 포지션 초기화\n",
    "    position = 'None'\n",
    "\n",
    "    # 실제 및 예측 주식 가격 설정\n",
    "    actual_prices = filtered_df.set_index('ds')['y']\n",
    "    predicted_prices = filtered_df_forecast.set_index('ds')['yhat']\n",
    "\n",
    "    # 거래 기록 초기화\n",
    "    trading_history = []\n",
    "\n",
    "    # 시뮬레이션 시작\n",
    "    for date, today_price, predicted_next_day_price in zip(actual_prices.index, actual_prices.values, predicted_prices.shift(-1).values):\n",
    "        predicted_change = predicted_next_day_price - today_price\n",
    "\n",
    "        # 매수 조건\n",
    "        if predicted_change > 0 and position != 'Hold':\n",
    "            shares_to_buy = current_capital / today_price\n",
    "            shares_owned += shares_to_buy\n",
    "            current_capital -= shares_to_buy * today_price\n",
    "            position = 'Hold'\n",
    "            trading_history.append('Buy')\n",
    "\n",
    "        # 매도 조건\n",
    "        elif predicted_change < 0 and position == 'Hold':\n",
    "            current_capital += shares_owned * today_price\n",
    "            shares_owned = 0\n",
    "            position = 'None'\n",
    "            trading_history.append('Cell')\n",
    "\n",
    "        # 보유 조건\n",
    "        elif position == 'Hold' and abs(predicted_change) <= 0.01 * today_price:\n",
    "            trading_history.append('Hold')\n",
    "\n",
    "        # 조치 없음 또는 보유\n",
    "        else:\n",
    "            if position == 'Hold':\n",
    "                trading_history.append('Hold')\n",
    "            else:\n",
    "                trading_history.append('Stay')#조치없음\n",
    "\n",
    "    # 최종 자본 및 수익률 계산\n",
    "\n",
    "    # trading_history의 마지막 인덱스 반환\n",
    "    last_record = trading_history[-1]\n",
    "    return last_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d220e94c-6d41-4857-9c2e-4d29bb5cd63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\chan\\anaconda3\\envs\\ng\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2688: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "998it [01:06, 15.09it/s]\n",
      "C:\\Users\\chan\\AppData\\Local\\Temp\\ipykernel_4060\\1829546400.py:130: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_df = result_df.append({'date': date, 'predict': 1.0}, ignore_index=True)\n",
      "C:\\Users\\chan\\AppData\\Local\\Temp\\ipykernel_4060\\1829546400.py:130: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_df = result_df.append({'date': date, 'predict': 1.0}, ignore_index=True)\n",
      "C:\\Users\\chan\\AppData\\Local\\Temp\\ipykernel_4060\\1829546400.py:130: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_df = result_df.append({'date': date, 'predict': 1.0}, ignore_index=True)\n",
      "C:\\Users\\chan\\AppData\\Local\\Temp\\ipykernel_4060\\1829546400.py:130: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_df = result_df.append({'date': date, 'predict': 1.0}, ignore_index=True)\n",
      "C:\\Users\\chan\\AppData\\Local\\Temp\\ipykernel_4060\\1829546400.py:130: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_df = result_df.append({'date': date, 'predict': 1.0}, ignore_index=True)\n",
      "C:\\Users\\chan\\AppData\\Local\\Temp\\ipykernel_4060\\1829546400.py:130: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_df = result_df.append({'date': date, 'predict': 1.0}, ignore_index=True)\n",
      "C:\\Users\\chan\\AppData\\Local\\Temp\\ipykernel_4060\\1829546400.py:130: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_df = result_df.append({'date': date, 'predict': 1.0}, ignore_index=True)\n",
      "C:\\Users\\chan\\AppData\\Local\\Temp\\ipykernel_4060\\1829546400.py:130: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_df = result_df.append({'date': date, 'predict': 1.0}, ignore_index=True)\n",
      "C:\\Users\\chan\\AppData\\Local\\Temp\\ipykernel_4060\\1829546400.py:130: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_df = result_df.append({'date': date, 'predict': 1.0}, ignore_index=True)\n",
      "C:\\Users\\chan\\AppData\\Local\\Temp\\ipykernel_4060\\1829546400.py:130: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_df = result_df.append({'date': date, 'predict': 1.0}, ignore_index=True)\n",
      "C:\\Users\\chan\\AppData\\Local\\Temp\\ipykernel_4060\\1829546400.py:130: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_df = result_df.append({'date': date, 'predict': 1.0}, ignore_index=True)\n",
      "C:\\Users\\chan\\AppData\\Local\\Temp\\ipykernel_4060\\1829546400.py:130: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_df = result_df.append({'date': date, 'predict': 1.0}, ignore_index=True)\n",
      "C:\\Users\\chan\\AppData\\Local\\Temp\\ipykernel_4060\\1829546400.py:130: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_df = result_df.append({'date': date, 'predict': 1.0}, ignore_index=True)\n",
      "C:\\Users\\chan\\AppData\\Local\\Temp\\ipykernel_4060\\1829546400.py:130: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_df = result_df.append({'date': date, 'predict': 1.0}, ignore_index=True)\n",
      "C:\\Users\\chan\\AppData\\Local\\Temp\\ipykernel_4060\\1829546400.py:130: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_df = result_df.append({'date': date, 'predict': 1.0}, ignore_index=True)\n",
      "C:\\Users\\chan\\AppData\\Local\\Temp\\ipykernel_4060\\1829546400.py:130: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_df = result_df.append({'date': date, 'predict': 1.0}, ignore_index=True)\n",
      "C:\\Users\\chan\\AppData\\Local\\Temp\\ipykernel_4060\\1829546400.py:130: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_df = result_df.append({'date': date, 'predict': 1.0}, ignore_index=True)\n",
      "C:\\Users\\chan\\AppData\\Local\\Temp\\ipykernel_4060\\1829546400.py:130: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_df = result_df.append({'date': date, 'predict': 1.0}, ignore_index=True)\n",
      "C:\\Users\\chan\\AppData\\Local\\Temp\\ipykernel_4060\\1829546400.py:130: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_df = result_df.append({'date': date, 'predict': 1.0}, ignore_index=True)\n",
      "C:\\Users\\chan\\AppData\\Local\\Temp\\ipykernel_4060\\1829546400.py:130: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_df = result_df.append({'date': date, 'predict': 1.0}, ignore_index=True)\n",
      "C:\\Users\\chan\\AppData\\Local\\Temp\\ipykernel_4060\\1829546400.py:130: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_df = result_df.append({'date': date, 'predict': 1.0}, ignore_index=True)\n",
      "C:\\Users\\chan\\AppData\\Local\\Temp\\ipykernel_4060\\1829546400.py:130: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_df = result_df.append({'date': date, 'predict': 1.0}, ignore_index=True)\n",
      "C:\\Users\\chan\\AppData\\Local\\Temp\\ipykernel_4060\\1829546400.py:130: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_df = result_df.append({'date': date, 'predict': 1.0}, ignore_index=True)\n",
      "C:\\Users\\chan\\AppData\\Local\\Temp\\ipykernel_4060\\1829546400.py:130: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_df = result_df.append({'date': date, 'predict': 1.0}, ignore_index=True)\n",
      "C:\\Users\\chan\\AppData\\Local\\Temp\\ipykernel_4060\\1829546400.py:130: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_df = result_df.append({'date': date, 'predict': 1.0}, ignore_index=True)\n",
      "C:\\Users\\chan\\AppData\\Local\\Temp\\ipykernel_4060\\1829546400.py:130: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_df = result_df.append({'date': date, 'predict': 1.0}, ignore_index=True)\n",
      "C:\\Users\\chan\\AppData\\Local\\Temp\\ipykernel_4060\\1829546400.py:130: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_df = result_df.append({'date': date, 'predict': 1.0}, ignore_index=True)\n",
      "C:\\Users\\chan\\AppData\\Local\\Temp\\ipykernel_4060\\1829546400.py:130: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_df = result_df.append({'date': date, 'predict': 1.0}, ignore_index=True)\n",
      "C:\\Users\\chan\\AppData\\Local\\Temp\\ipykernel_4060\\1829546400.py:130: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_df = result_df.append({'date': date, 'predict': 1.0}, ignore_index=True)\n",
      "C:\\Users\\chan\\AppData\\Local\\Temp\\ipykernel_4060\\1829546400.py:130: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_df = result_df.append({'date': date, 'predict': 1.0}, ignore_index=True)\n",
      "C:\\Users\\chan\\AppData\\Local\\Temp\\ipykernel_4060\\1829546400.py:130: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_df = result_df.append({'date': date, 'predict': 1.0}, ignore_index=True)\n",
      "C:\\Users\\chan\\AppData\\Local\\Temp\\ipykernel_4060\\1829546400.py:130: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_df = result_df.append({'date': date, 'predict': 1.0}, ignore_index=True)\n",
      "C:\\Users\\chan\\AppData\\Local\\Temp\\ipykernel_4060\\1829546400.py:130: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_df = result_df.append({'date': date, 'predict': 1.0}, ignore_index=True)\n",
      "C:\\Users\\chan\\AppData\\Local\\Temp\\ipykernel_4060\\1829546400.py:130: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_df = result_df.append({'date': date, 'predict': 1.0}, ignore_index=True)\n",
      "C:\\Users\\chan\\AppData\\Local\\Temp\\ipykernel_4060\\1829546400.py:130: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_df = result_df.append({'date': date, 'predict': 1.0}, ignore_index=True)\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "C:\\Users\\chan\\AppData\\Local\\Temp\\ipykernel_4060\\1829546400.py:178: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_filtered_future = result_filtered_future.append(new_row, ignore_index=True)\n",
      "17:33:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:33:07 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell\n"
     ]
    }
   ],
   "source": [
    "ticker = 'AVGO'\n",
    "start_date = '2021-04-06'\n",
    "end_date = '2024-05-14'\n",
    "end_date_future = '2024-05-15'\n",
    "initial_capital = 10000000000.0\n",
    "state = simulate_trading(ticker, start_date, end_date, end_date_future, initial_capital)\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4490a656-1a20-40a5-ac9c-29390b171b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\chan\\anaconda3\\envs\\ng\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2688: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "6it [00:01,  4.25it/s]\n",
      "C:\\Users\\chan\\AppData\\Local\\Temp\\ipykernel_17836\\850901545.py:155: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result_df_1['date'] = result_df_1['date'].dt.date\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            date  predict\n",
      "0     2021-04-05    1.000\n",
      "1     2021-04-06    1.000\n",
      "2     2021-04-07    1.000\n",
      "3     2021-04-08    1.000\n",
      "4     2021-04-09    1.000\n",
      "...          ...      ...\n",
      "1133  2024-05-12    0.500\n",
      "1134  2024-05-13    0.917\n",
      "1135  2024-05-14    1.126\n",
      "1136  2024-05-15    1.000\n",
      "1137  2024-05-16    1.000\n",
      "\n",
      "[1138 rows x 2 columns]\n",
      "           date  predict\n",
      "0    2021-04-05    1.000\n",
      "1    2021-04-06    1.000\n",
      "2    2021-04-07    1.000\n",
      "3    2021-04-08    1.000\n",
      "4    2021-04-09    1.000\n",
      "...         ...      ...\n",
      "1133 2024-05-12    0.500\n",
      "1134 2024-05-13    0.917\n",
      "1135 2024-05-14    1.126\n",
      "1136 2024-05-15    1.000\n",
      "1137 2024-05-16    1.000\n",
      "\n",
      "[1138 rows x 2 columns]\n",
      "            ds            y  predict\n",
      "0   2021-04-05   488.480011    1.000\n",
      "1   2021-04-06   483.869995    1.000\n",
      "2   2021-04-07   482.459991    1.000\n",
      "3   2021-04-08   485.480011    1.000\n",
      "4   2021-04-09   485.089996    1.000\n",
      "..         ...          ...      ...\n",
      "781 2024-05-10  1332.800049    1.100\n",
      "782 2024-05-13  1337.510010    0.917\n",
      "783 2024-05-14  1380.030029    1.126\n",
      "784 2024-05-15  1436.170044    1.000\n",
      "785 2024-05-16  1441.069946    1.000\n",
      "\n",
      "[786 rows x 3 columns]           date  predict\n",
      "0   2021-04-05    1.000\n",
      "1   2021-04-06    1.000\n",
      "2   2021-04-07    1.000\n",
      "3   2021-04-08    1.000\n",
      "4   2021-04-09    1.000\n",
      "..         ...      ...\n",
      "782 2024-05-13    0.917\n",
      "783 2024-05-14    1.126\n",
      "784 2024-05-15    1.000\n",
      "785 2024-05-16    1.000\n",
      "786 2024-05-17    1.020\n",
      "\n",
      "[787 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chan\\AppData\\Local\\Temp\\ipykernel_17836\\850901545.py:233: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_filtered_future = result_filtered_future.append(new_row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "\n",
    "import pandas_datareader.data as web # 주식 데이터를 얻어오기 위해 사용\n",
    "import datetime # 시간 처리\n",
    "import yfinance as yf\n",
    "import FinanceDataReader as fdr\n",
    "from prophet import Prophet\n",
    "# %matplotlib inline\n",
    "\n",
    "ticker = 'AVGO'\n",
    "start_date = '2021-04-05'\n",
    "end_date = '2024-05-16'\n",
    "counter = 1\n",
    "# def bertcode_predict(ticker, start_date, end_date_1,counter):\n",
    "ticker = ticker\n",
    "counter = counter\n",
    "#start_date 21-04-06 / end_date_1 24-05-09\n",
    "start_date = start_date\n",
    "end_date = end_date\n",
    "\n",
    "start_year, start_month, start_day = start_date.split('-')\n",
    "end_year, end_month, end_day = end_date.split('-')\n",
    "\n",
    "start_year = int(start_year)\n",
    "start_month = int(start_month)\n",
    "start_day = int(start_day)\n",
    "\n",
    "end_year = int(end_year)\n",
    "end_month = int(end_month)\n",
    "end_day = int(end_day)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "max_seq_len = 128\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "path = f\"C:/Users/chan/Documents/GitHub/ML/test_{ticker}/\"\n",
    "# path = \"C:/Users/chan/Documents/GitHub/ML/test10_AVGO/\"\n",
    "news_df = pd.DataFrame(columns=[\"title\", \"content\"])\n",
    "\n",
    "\n",
    "for txts in os.listdir(path):\n",
    "    full_path = os.path.join(path, txts)  # 파일 전체 경로 생성\n",
    "    if full_path.endswith('header.txt'):\n",
    "        continue\n",
    "    if os.path.isfile(full_path):  # 파일인지 확인\n",
    "        with open(full_path, \"r\", encoding=\"utf-8\") as txt_file:\n",
    "            title = txt_file.readline().strip()\n",
    "            content = txt_file.read().replace('\\n', ' ')\n",
    "            # DataFrame 생성 후 concat 함수를 사용하여 추가\n",
    "            new_row = pd.DataFrame({\"title\": [title], \"content\": [content]})\n",
    "            news_df = pd.concat([news_df, new_row], ignore_index=True)\n",
    "inputs = [content for content in news_df['content']]\n",
    "\n",
    "# 입력 데이터를 BERT 모델의 입력 형식에 맞게 변환\n",
    "max_length = 128\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for content in inputs:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        content,                    # content\n",
    "                        add_special_tokens = True,  # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = max_length,           # Pad & truncate all sentences\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors\n",
    "                   )\n",
    "    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "predict_model = torch.load(\"bert_model_loss0.34.pth\", map_location=torch.device('cpu'))\n",
    "predict_model.to(torch.device('cpu'))\n",
    "\n",
    "predicted_labels = []\n",
    "\n",
    "for inputs in tqdm(zip(input_ids, attention_masks)):\n",
    "    input_ids = inputs[0].to(torch.device('cpu'))\n",
    "    attention_mask = inputs[1].to(torch.device('cpu'))\n",
    "    output = predict_model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    ps = F.softmax(output.logits, dim=1)\n",
    "    top_p, top_class = ps.topk(1, dim=1)\n",
    "    predicted_labels.append(top_class.item())\n",
    "\n",
    "predict_df = pd.DataFrame({'predicted_label': predicted_labels})\n",
    "news_df[\"predict\"] = predict_df\n",
    "news_df.to_csv('predicted_news.csv', index=False)\n",
    "\n",
    "news_df['date'] = pd.to_datetime(news_df['title']).dt.date  # 날짜 추출\n",
    "news_df['time'] = pd.to_datetime(news_df['title']).dt.time  # 시간 추출\n",
    "\n",
    "# 날짜별로 그룹화하고, 'predict' 열에 대한 평균을 계산\n",
    "grouped_df = news_df.groupby('date').agg({\n",
    "    'title': lambda x: x.tolist(),\n",
    "    'content': lambda x: x.tolist(),\n",
    "    'predict': lambda x: round(x.mean(), 3)  \n",
    "})\n",
    "\n",
    "grouped_df.reset_index(inplace=True)  # 인덱스 리셋\n",
    "\n",
    "# 결과 출력\n",
    "grouped_df[['date', 'title', 'content', 'predict']]\n",
    "\n",
    "result_df = grouped_df[['date', 'predict']]\n",
    "\n",
    "news_df['date'] = pd.to_datetime(news_df['title']).dt.date  # 날짜 추출\n",
    "news_df['time'] = pd.to_datetime(news_df['title']).dt.time  # 시간 추출\n",
    "\n",
    "# 'date' 열을 datetime 형식으로 다시 변환\n",
    "news_df['date'] = pd.to_datetime(news_df['date'])\n",
    "\n",
    "# 날짜별로 그룹화하고, 'predict' 열에 대한 평균을 계산\n",
    "grouped_df = news_df.groupby('date').agg({\n",
    "    'title': lambda x: x.tolist(),\n",
    "    'content': lambda x: x.tolist(),\n",
    "    'predict': lambda x: round(x.mean(), 3)  \n",
    "})\n",
    "\n",
    "grouped_df.reset_index(inplace=True)  # 인덱스 리셋\n",
    "\n",
    "# 모든 날짜를 포함하는 날짜 범위 생성\n",
    "all_dates = pd.date_range(start=grouped_df['date'].min(), end=grouped_df['date'].max(), freq='D')\n",
    "# 새로운 DataFrame 생성 후 기존 데이터와 병합\n",
    "complete_df = pd.DataFrame(all_dates, columns=['date'])\n",
    "complete_df['date'] = pd.to_datetime(complete_df['date'])  # 날짜를 datetime으로 변환\n",
    "complete_df = complete_df.merge(grouped_df, on='date', how='left')\n",
    "# 누락된 'predict' 값을 이전 값으로 채우기\n",
    "complete_df['predict'] = complete_df['predict'].fillna(method='ffill')\n",
    "\n",
    "# 'date'와 'predict' 열만 선택하여 결과 출력\n",
    "result_df_1 = complete_df[['date', 'predict']]\n",
    "\n",
    "##############################################\n",
    "csv_filename = f\"news_data_{ticker}.csv\"\n",
    "loaded_df = pd.read_csv(csv_filename)\n",
    "\n",
    "result_df_1['date'] = result_df_1['date'].dt.date\n",
    "result_df = pd.concat([loaded_df, result_df_1], ignore_index=True)\n",
    "print(result_df)\n",
    "\n",
    "###############################################\n",
    "\n",
    "start_date = datetime.datetime( start_year, start_month, start_day )\n",
    "end_date = datetime.datetime( end_year, end_month, end_day)\n",
    "\n",
    "# 주어진 범위 내에 있는 날짜 생성\n",
    "# missing_dates = pd.date_range(start=start_date, end=end_date).difference(result_df['date'])\n",
    "\n",
    "# # 새로운 날짜와 예측값 추가\n",
    "# for date in missing_dates:\n",
    "#     result_df = result_df.append({'date': date, 'predict': 1.0}, ignore_index=True)\n",
    "\n",
    "# 날짜 기준으로 데이터프레임 정렬\n",
    "result_df['date'] = pd.to_datetime(result_df['date'])\n",
    "result_df = result_df.sort_values('date').reset_index(drop=True)\n",
    "print(result_df)\n",
    "\n",
    "if counter == 6:\n",
    "    result_df.to_csv(csv_filename, index=False)\n",
    "    \n",
    "def makeStockChart(code, sDay, eDay):\n",
    "    data = yf.download(code, start=sDay, end=eDay)\n",
    "\n",
    "code_name = ticker\n",
    "makeStockChart(code_name, start_date, end_date) #기간받아오기\n",
    "\n",
    "datas = yf.download(code_name, start=start_date, end=end_date)\n",
    "\n",
    "dic = {\n",
    "    'ds' : datas.index,\n",
    "    'y' : datas.Close,\n",
    "    # 'volume': datas.Volume\n",
    "    \n",
    "}\n",
    "\n",
    "finance_df = pd.DataFrame( dic )\n",
    "\n",
    "#인덱스 초기화(원본 까지 적용)\n",
    "finance_df.reset_index( inplace=True )\n",
    "\n",
    "del finance_df['Date']\n",
    "\n",
    "#########################\n",
    "stock = yf.Ticker(ticker)\n",
    "hist = stock.history(period = '1d',interval=\"1h\")\n",
    "last_row = finance_df.iloc[-1]  # 마지막 행 가져오기\n",
    "next_date = last_row['ds'] + pd.DateOffset(days=1)\n",
    "second_price = hist['Close'][counter]# 1 -> 10시반 / 3 -> 12시반 / 5 -> 2시반 / 7 -> 4시반\n",
    "start_next_row = pd.DataFrame({'ds': [next_date], 'y': second_price })\n",
    "finance_df = pd.concat([finance_df, start_next_row], ignore_index=True)\n",
    "\n",
    "#########################\n",
    "\n",
    "date_range = pd.date_range(start=start_date, end=end_date)\n",
    "df_date_range = pd.DataFrame(date_range, columns=['date'])\n",
    "df_filtered = df_date_range[~df_date_range['date'].isin(finance_df['ds'])]\n",
    "\n",
    "common_dates = result_df[result_df['date'].isin(df_filtered['date'])]['date']\n",
    "result_filtered = result_df[~result_df['date'].isin(common_dates)]\n",
    "\n",
    "result_filtered.reset_index(inplace=True)\n",
    "\n",
    "result_filtered_future = result_filtered.copy()\n",
    "result_filtered_future = result_filtered_future.drop(columns=['index'])\n",
    "\n",
    "# 'predict' 열에만 평균 값 추가\n",
    "mean_last_seven = result_filtered_future['predict'].iloc[-7:].mean()\n",
    "# mean_last_seven_1 = result_filtered_future['volume'].iloc[-7:].mean()\n",
    "# 다음 날짜 계산\n",
    "last_date = result_filtered_future['date'].iloc[-1]\n",
    "next_date = pd.to_datetime(last_date) + pd.DateOffset(days=1)\n",
    "\n",
    "# 데이터프레임에 새로운 행 추가\n",
    "new_row = {'date': next_date, 'predict': round(mean_last_seven, 3)}  # 소수점 세 자리까지 반올림\n",
    "result_filtered_future = result_filtered_future.append(new_row, ignore_index=True)\n",
    "\n",
    "# 'predict' 열의 소수점 자리 수 설정\n",
    "result_filtered_future['predict'] = result_filtered_future['predict'].round(3)\n",
    "\n",
    "finance_df = pd.concat([finance_df, result_filtered['predict']], axis=1)\n",
    "print(finance_df,result_filtered_future)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4fd569-c448-4457-84fb-ee7e2b11e06c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
